cd ~/Projects/mote1/data/16s_data/hs_16s

# QIIME2 workflow
source activate qiime2-2019.1

## Import data
qiime tools import \
	--type 'SampleData[PairedEndSequencesWithQuality]' \
	--input-path data/16S/manifest.csv \
	--output-path data/16S/intermediate/paired-end-demux.qza \
	--input-format PairedEndFastqManifestPhred33

qiime demux summarize \
	--i-data data/16S/intermediate/paired-end-demux.qza \
	--o-visualization data/16S/intermediate/paired-end-demux.qzv \
	--verbose

# Run DADA2 pipeline and trim primers. This takes about 15 minutes.
# amplicon with 515FB and 806RB is expected ~390 bp
# trim the primer sequences
# n-threads 0 uses all available cores
qiime dada2 denoise-paired \
	--i-demultiplexed-seqs data/16S/intermediate/paired-end-demux.qza \
	--p-trim-left-f 19 \
	--p-trim-left-r 20 \
	--p-trunc-len-f 0 \
	--p-trunc-len-r 0 \
	--p-n-threads 0 \
	--o-denoising-stats data/16S/intermediate/denoising-stats.qza \
	--o-representative-sequences data/16S/intermediate/rep-seqs.qza \
	--o-table data/16S/intermediate/table.qza

# Summarize feature table
qiime feature-table summarize \
	--i-table data/16S/intermediate/table.qza \
	--o-visualization data/16S/intermediate/table.qzv

## Samples have between 2800 and 14400 reads, median is 7200.
## Features have between 2 and 31526 reads.

qiime feature-table tabulate-seqs \
  --i-data data/16S/intermediate/rep-seqs.qza \
  --o-visualization data/16S/intermediate/rep-seqs.qzv

### Train classifier
#Next import the silva database sequences (.fasta file) and the taxonomy (.txt file) and change to .qza artifact
#Because we use the v4/v5 regions, it is useful to download the full 16S dataset
#Also the newest version of SILVA is now 128 (I think we were using 119 previously...) 
qiime tools import \
  --type 'FeatureData[Sequence]' \
  --input-path data/SILVA_132_QIIME_release/rep_set/rep_set_16S_only/99/silva_132_99_16S.fna \
  --output-path data/16S/intermediate/silva_bac.qza

qiime tools import \
  --type 'FeatureData[Taxonomy]' \
  --input-format HeaderlessTSVTaxonomyFormat \
  --input-path data/SILVA_132_QIIME_release/taxonomy/16S_only/99/consensus_taxonomy_all_levels.txt \
  --output-path data/16S/intermediate/ref-taxonomy.qza

##Extract the reference reads from SILVA for your dataset - this takes a long time!
#This is based on your primer set that you use. 
#The following code is based on 515F/806R primers, but you can substitute for any other primer set
# took out: --p-trunc-len 300
qiime feature-classifier extract-reads \
  --i-sequences data/16S/intermediate/silva_bac.qza \
  --p-f-primer GTGYCAGCMGCCGCGGTAA \
  --p-r-primer GGACTACNVGGGTWTCTAAT \
  --o-reads data/16S/intermediate/ref-seqs.qza
#Next, train a "Naive Bayes classifier" using the reference sequences we just extracted above
qiime feature-classifier fit-classifier-naive-bayes \
  --i-reference-reads data/16S/intermediate/ref-seqs.qza \
  --i-reference-taxonomy data/16S/intermediate/ref-taxonomy.qza \
  --o-classifier data/16S/intermediate/classifier_silva_v4.qza

# Assign taxonomy
# Use trained classifier silva 515F/806R downloaded from https://docs.qiime2.org/2019.1/data-resources/
qiime feature-classifier classify-sklearn \
	--i-classifier data/16S/intermediate/classifier_silva_v4.qza \
	--i-reads data/16S/intermediate/rep-seqs.qza \
	--p-n-jobs -1 \
	--o-classification data/16S/intermediate/taxonomy.qza

qiime metadata tabulate \
  --m-input-file data/16S/intermediate/taxonomy.qza \
  --o-visualization data/16S/intermediate/taxonomy.qzv

### Filtering table:  mitochondria and chloroplast reads, PCR/sequencing errors, etc###

qiime taxa filter-table \
  --i-table data/16S/intermediate/table.qza \
  --i-taxonomy data/16S/intermediate/taxonomy.qza \
  --p-exclude mitochondria,chloroplast \
  --o-filtered-table data/16S/intermediate/filtered_table_mitchl.qza

qiime feature-table summarize \
  --i-table data/16S/intermediate/filtered_table_mitchl.qza \
  --o-visualization data/16S/intermediate/filtered_table_mitchl.qzv

##Generate a tree for phylogenetic analyses##
#We first align using MAFFT

qiime alignment mafft \
  --i-sequences data/16S/intermediate/rep-seqs.qza \
  --o-alignment data/16S/intermediate/aligned-rep-seqs.qza

#Next we mask(or filter) the alignment to remove highly variable positions aka noise

qiime alignment mask \
  --i-alignment data/16S/intermediate/aligned-rep-seqs.qza \
  --o-masked-alignment data/16S/intermediate/masked-aligned-rep-seqs.qza

#Next we apply FastTree to make the tree from the masked alignment

qiime phylogeny fasttree \
  --i-alignment data/16S/intermediate/masked-aligned-rep-seqs.qza \
  --o-tree data/16S/intermediate/unrooted-tree.qza

##FastTree creates an unrooted tree, so now we need to apply a midpoint root

qiime phylogeny midpoint-root \
  --i-tree data/16S/intermediate/unrooted-tree.qza \
  --o-rooted-tree data/16S/intermediate/rooted-tree.qza


##Next, create bar plots
##When you view your .qzv file on the view.qiime2.org website, you can download the .csv
##of your data at whichever taxonomic level you would like. 
##This .csv can be imported directly into R for analyses

qiime taxa barplot \
  --i-table data/16S/intermediate/filtered_table_mitchl.qza \
  --i-taxonomy data/16S/intermediate/taxonomy.qza \
  --m-metadata-file data/16S/sample_metadata.txt \
  --o-visualization data/16S/intermediate/hs_taxa-bar-plots.qzv





### Export for use in phyloseq###

qiime tools export \
	--input-path data/16S/intermediate/rooted-tree.qza \
	--output-path data/16S/processed

qiime tools export \
	--input-path data/16S/intermediate/taxonomy.qza \
	--output-path data/16S/processed

qiime tools export \
	--input-path data/16S/intermediate/filtered_table_mitchl.qza \
	--output-path data/16S/processed

biom convert -i data/16S/processed/feature-table.biom -o data/16S/processed/feature-table.tsv --to-tsv



# filter the taxonomy table to only include OTUs present in the OTU table (mitchl filtered out)
grep -f <(cut -f1 data/16S/processed/feature-table.tsv) data/16S/processed/taxonomy.tsv > data/16S/processed/taxonomy-filtered.tsv

# Append the taxonomy as a column to the feature table, then convert back to biom for import to phyloseq
paste <(tail -n +2 data/16S/processed/feature-table.tsv | sort) <(echo "taxonomy" | cat - <(sort data/16S/processed/taxonomy-filtered.tsv | cut -f2)) > data/16S/processed/feature-table-taxon.txt

biom convert -i data/16S/processed/feature-table-taxon.txt -o data/16S/processed/feature-table-taxon.biom --to-hdf5 --table-type="OTU table" --process-obs-metadata taxonomy

